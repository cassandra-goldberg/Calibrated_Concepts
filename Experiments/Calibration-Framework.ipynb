{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2e90c1d8-e0d2-46d4-9c23-941b88cba68d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311be61d-5189-4677-8d0e-4a0e8b1ed4b9",
   "metadata": {},
   "source": [
    "# Get Data and Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d0eefab3-cf46-413c-88fb-b2e4af656ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'CUB'\n",
    "concept_name = 'has_back_color::black'\n",
    "\n",
    "metadata_df = pd.read_csv(f'../Data/{dataset_name}/metadata.csv')\n",
    "embeddings = torch.load(f'Embeddings/{dataset_name}/embeddings.pt') \n",
    "\n",
    "train_indices = metadata_df[metadata_df['split'] == 'train'].index.tolist()\n",
    "test_indices = metadata_df[metadata_df['split'] == 'test'].index.tolist()\n",
    "calibration_indices = metadata_df[metadata_df['split'] == 'calibration'].index.tolist()\n",
    "\n",
    "X_train = embeddings[train_indices].numpy()\n",
    "X_test = embeddings[test_indices].numpy()\n",
    "X_cal = embeddings[calibration_indices].numpy()\n",
    "\n",
    "y_train = np.array(metadata_df[(metadata_df['split'] == 'train')][concept_name])\n",
    "y_test = np.array(metadata_df[(metadata_df['split'] == 'test')][concept_name])\n",
    "y_cal = np.array(metadata_df[(metadata_df['split'] == 'calibration')][concept_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2412e8d6-180e-4af7-9348-63d379484558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for now, train a basic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e8485764-bd26-4c16-9469-36ec7600517b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 74.69%\n"
     ]
    }
   ],
   "source": [
    "base_model = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=200, random_state=42)\n",
    "base_model.fit(X_train, y_train)\n",
    "y_pred = base_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a8763-21f7-40b2-8093-5cbd92b3aa91",
   "metadata": {},
   "source": [
    "## Platt Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "798ddeca-3d5d-48b1-97c6-8da62e932908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.70%\n"
     ]
    }
   ],
   "source": [
    "platt_calibrated_model = CalibratedClassifierCV(base_model, method='sigmoid', cv='prefit')\n",
    "platt_calibrated_model.fit(X_cal, y_cal)  # Fit calibration on validation set\n",
    "\n",
    "y_probs = platt_calibrated_model.predict_proba(X_test)\n",
    "y_pred = np.argmax(y_probs, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782f7bb-7694-4061-9754-2765f018083b",
   "metadata": {},
   "source": [
    "## Isotonic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "49d38f24-380b-48fd-8e84-e9f88d277b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.65%\n"
     ]
    }
   ],
   "source": [
    "isotonic_calibrated_model = CalibratedClassifierCV(base_model, method='isotonic', cv='prefit') \n",
    "isotonic_calibrated_model.fit(X_cal, y_cal)\n",
    "\n",
    "y_probs = isotonic_calibrated_model.predict_proba(X_test)\n",
    "y_pred = np.argmax(y_probs, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0c4bb-e38a-46c4-a89f-884ff4ac8094",
   "metadata": {},
   "source": [
    "## Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e28cfe27-81c5-472d-aae9-aa3b81064a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 4.3254425e-10]\n",
      " [9.9957806e-01 4.2194437e-04]\n",
      " [9.8677015e-01 1.3229854e-02]\n",
      " [9.9973083e-01 2.6914920e-04]\n",
      " [1.0000000e+00 3.4185834e-09]]\n",
      "[[1.0000000e+00 4.3254425e-10]\n",
      " [9.9957806e-01 4.2194437e-04]\n",
      " [9.8677015e-01 1.3229854e-02]\n",
      " [9.9973083e-01 2.6914920e-04]\n",
      " [1.0000000e+00 3.4185834e-09]]\n",
      "Optimal temperature: 10.0000\n",
      "[[1.0000000e+00 1.9171861e-10]\n",
      " [1.0000000e+00 1.7745098e-08]\n",
      " [9.9999529e-01 4.6975442e-06]\n",
      " [9.9957001e-01 4.3001771e-04]\n",
      " [9.9999952e-01 4.9182859e-07]]\n",
      "Test Accuracy: 74.69%\n"
     ]
    }
   ],
   "source": [
    "class TemperatureScaling(nn.Module):\n",
    "    \"\"\"A simple module for temperature scaling.\"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super(TemperatureScaling, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "    def predict_scaled_logits(self, X):\n",
    "        \"\"\"Scale the logits using the current temperature.\"\"\"\n",
    "        original_logits = self.base_model.predict_proba(X) \n",
    "        print(original_logits[:5])\n",
    "        scaled_logits = original_logits / self.temperature  # Scale by the temperature\n",
    "        return scaled_logits\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return the probability predictions from the scaled logits.\"\"\"\n",
    "        scaled_logits = self.predict_scaled_logits(X)  # Get scaled logits\n",
    "        exps = np.exp(scaled_logits - np.max(scaled_logits, axis=1, keepdims=True))  # Stability trick for softmax\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)  # Normalize to get probabilities\n",
    "\n",
    "def train_temperature_scaling(base_model, X_cal, y_cal):\n",
    "    \"\"\"Train temperature scaling using negative log-likelihood. \"\"\"\n",
    "    \n",
    "    temperature_model = TemperatureScaling(base_model)  # Initialize the temperature scaling model\n",
    "    \n",
    "    def nll_loss(temperature):\n",
    "        \"\"\"Negative log-likelihood loss for the given temperature.\"\"\"\n",
    "        temperature_model.temperature = temperature[0]  # Update temperature\n",
    "        scaled_logits = temperature_model.predict_scaled_logits(X_cal)  # Get scaled logits\n",
    "        #print(f\"Scaled logits for temperature {temperature[0]}: {scaled_logits[:5]}\") \n",
    "        # Compute cross-entropy loss\n",
    "        log_probs = scaled_logits - np.log(np.sum(np.exp(scaled_logits), axis=1, keepdims=True))\n",
    "        nll = -np.mean(log_probs[np.arange(len(y_cal)), y_cal])  # Negative log-likelihood\n",
    "        #print(\"Current loss:\", nll)\n",
    "        return nll\n",
    "    \n",
    "    # Minimize the negative log-likelihood loss to find the optimal temperature\n",
    "    result = minimize(nll_loss, x0=[10], bounds=[(1e-2, 10.0)], tol=1, method='L-BFGS-B')  # Temperature > 0\n",
    "    \n",
    "    # To visualize the loss for different temperatures\n",
    "    # temperatures = np.linspace(0.1, 10, 100)\n",
    "    # losses = [nll_loss([t]) for t in temperatures]\n",
    "    # plt.plot(temperatures, losses)\n",
    "    # plt.xlabel('Temperature')\n",
    "    # plt.ylabel('Negative Log-Likelihood')\n",
    "    # plt.show()\n",
    "    \n",
    "    # Store the optimal temperature\n",
    "    temperature_model.temperature = result.x[0]  \n",
    "    print(f\"Optimal temperature: {temperature_model.temperature:.4f}\")\n",
    "    \n",
    "    return temperature_model\n",
    "\n",
    "temperature_calibrated_model = train_temperature_scaling(base_model, X_cal, y_cal)\n",
    "\n",
    "y_probs = temperature_calibrated_model.predict_proba(X_test)\n",
    "y_pred = np.argmax(y_probs, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
