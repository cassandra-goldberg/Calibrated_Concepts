{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90e159-ca72-4ab1-954b-d6c0bf5b27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoProcessor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae252295-5544-4edf-9d18-4b7fe4d503ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model and processor\n",
    "model_name = \"openai/clip-vit-large-patch14\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6fcab-b01a-4ef8-aa07-f4fa239422b9",
   "metadata": {},
   "source": [
    "# Embed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42221f99-2305-4095-a5bb-26a1d60772fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(images):\n",
    "    inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(pixel_values=inputs['pixel_values']) # Get the class token embedding \n",
    "    return image_features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce2c00-ce4f-40a3-9a06-404c8a44bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_embeddings(dataset_name='CLEVR', batch_size=100, only_train=True):\n",
    "    \"\"\"Computes image embeddings, centers, and normalizes them.\"\"\"\n",
    "    metadata = pd.read_csv(f'Data/{dataset_name}/metadata.csv')\n",
    "    if only_train:\n",
    "        metadata = metadata[metadata['split'] == 'train']\n",
    "    image_paths = metadata['image_path'].tolist()\n",
    "\n",
    "    # Load images\n",
    "    print(\"Loading images...\")\n",
    "    images = []\n",
    "    for image_filename in tqdm(image_paths):\n",
    "        image = Image.open(f'Data/{dataset_name}/{image_filename}').convert(\"RGB\")\n",
    "        image_tensor = torch.tensor(np.array(image).transpose((2, 0, 1)))  # Convert image to (C, H, W)\n",
    "        images.append(image_tensor)\n",
    "    \n",
    "    print(f\"Loaded {len(images)} images.\")\n",
    "\n",
    "    # Embed images in batches\n",
    "    print(\"Computing embeddings in batches...\")\n",
    "    embeddings = []\n",
    "    n_batches = (len(images) + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        batch_images = images[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_embeddings = get_image_embeddings(batch_images)  # Replace with your actual embedding function\n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    embeddings = torch.cat(embeddings, dim=0).cpu()  # Concatenate all batch embeddings\n",
    "    print(f\"Extracted embeddings of shape: {embeddings.shape}\")\n",
    "\n",
    "    # Center embeddings\n",
    "    print(\"Centering embeddings...\")\n",
    "    mean_embedding = embeddings.mean(dim=0)\n",
    "    centered_embeddings = embeddings - mean_embedding\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    norm_embeddings = centered_embeddings / centered_embeddings.norm(dim=0, keepdim=True)\n",
    "\n",
    "    # Save only the normalized embeddings to a tensor file\n",
    "    if only_train:\n",
    "        output_tensor_file = f'Experiments/Embeddings/{dataset_name}/train_embeddings.pt'\n",
    "    else:\n",
    "        output_tensor_file = f'Experiments/Embeddings/{dataset_name}/embeddings.pt'\n",
    "    torch.save(norm_embeddings, output_tensor_file)\n",
    "    print(f\"Normalized embeddings saved to {output_tensor_file} :)\")\n",
    "compute_image_embeddings(dataset_name='CUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c42ec2-03b8-4e20-95b3-b0f64beb9aca",
   "metadata": {},
   "source": [
    "# Compute Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950a9d7-c48a-4366-8aab-4c50f3210c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_concepts(dataset_name='CLEVR', only_train=True):\n",
    "    \"\"\"Computes concept vectors for each concept and saves them in a file.\"\"\"\n",
    "    output_dir = f'Experiments/Concepts/{dataset_name}'\n",
    "\n",
    "    # Load the embeddings and metadata\n",
    "    metadata_df = pd.read_csv(f'Data/{dataset_name}/metadata.csv')\n",
    "    if only_train:\n",
    "        embeddings = torch.load(f'Experiments/Embeddings/{dataset_name}/train_embeddings.pt')\n",
    "        metadata_df = metadata_df[metadata_df['split'] == 'train']\n",
    "    else:\n",
    "        embeddings = torch.load(f'Experiments/Embeddings/{dataset_name}/embeddings.pt')\n",
    "\n",
    "    # Infer concept columns (exclude 'image_path', 'class', and 'split')\n",
    "    concept_columns = [col for col in metadata_df.columns if col not in ['image_path', 'class', 'split']]\n",
    "    print(f\"Inferred concept columns: {concept_columns}\")\n",
    "    \n",
    "    # Dictionary to store all concept embeddings (merged across all concepts)\n",
    "    all_concept_embeddings = {}\n",
    "\n",
    "    # Iterate through each concept column\n",
    "    for concept in tqdm(concept_columns):\n",
    "        # Create a dictionary to store the embeddings for each concept value\n",
    "        concept_embeddings = {}\n",
    "\n",
    "        # Iterate through each image, its embedding, and the corresponding concept value\n",
    "        for idx, embedding in enumerate(embeddings):\n",
    "            concept_value = metadata_df.iloc[idx].loc[concept]\n",
    "            \n",
    "            # If the concept is present (binary check), add the embedding to the dictionary\n",
    "            if concept_value == 1:\n",
    "                concept_key = f'{concept}'  # This will be in the format 'concepttype::conceptkey'\n",
    "                if concept_key not in concept_embeddings:\n",
    "                    concept_embeddings[concept_key] = []\n",
    "                concept_embeddings[concept_key].append(embedding)\n",
    "\n",
    "        # Calculate the average embedding for each concept value\n",
    "        for value, group_embeddings in concept_embeddings.items():\n",
    "            group_embeddings = torch.stack(group_embeddings)  # Convert to tensor\n",
    "            avg_embedding = group_embeddings.mean(dim=0)  # Calculate the average embedding\n",
    "            avg_embedding = avg_embedding / avg_embedding.norm()  # Normalize the average embedding\n",
    "            \n",
    "            # Store the computed embedding\n",
    "            all_concept_embeddings[value] = avg_embedding\n",
    "\n",
    "    # Save **all concept embeddings** in a single file\n",
    "    if only_train:\n",
    "        output_file = os.path.join(output_dir, 'train_concepts.pt')\n",
    "    else:\n",
    "        output_file = os.path.join(output_dir, 'concepts.pt')\n",
    "    torch.save(all_concept_embeddings, output_file)\n",
    "    print(f\"All concept embeddings saved at {output_file} :)\")\n",
    "\n",
    "# Example usage:\n",
    "compute_concepts(dataset_name='CUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22020174-a94e-4d07-9ac8-74884b661f38",
   "metadata": {},
   "source": [
    "# Visualize Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348c230-a212-4b82-9aa7-3817a007c767",
   "metadata": {},
   "source": [
    "## Compute Cosine Similarities Between Images and Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20130564-82a0-4c4a-b8ac-4c0d83ed1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_sims(dataset_name='CLEVR', batch_size=32, only_train=True):\n",
    "    \"\"\"Compute cosine similarity between each image embedding and each concept.\"\"\"\n",
    "    \n",
    "    # Load the image and concept embeddings\n",
    "    #always compute cosine similarity using all embeddings\n",
    "    embeddings = torch.load(f'../Experiments/Embeddings/{dataset_name}/embeddings.pt') \n",
    "    if only_train:\n",
    "        all_concept_embeddings = torch.load(f'../Experiments/Concepts/{dataset_name}/train_concepts.pt')\n",
    "    else:\n",
    "        all_concept_embeddings = torch.load(f'../Experiments/Concepts/{dataset_name}/concepts.pt')\n",
    "\n",
    "    # Move embeddings and concept embeddings to GPU (if not already on GPU)\n",
    "    embeddings = embeddings.to('cuda') if not embeddings.is_cuda else embeddings\n",
    "    all_concept_embeddings = {k: v.to('cuda') if not v.is_cuda else v for k, v in all_concept_embeddings.items()}\n",
    "    \n",
    "    # Convert concept embeddings to a tensor (batch of all concept embeddings)\n",
    "    all_concept_embeddings_tensor = torch.stack(list(all_concept_embeddings.values())).to('cuda')\n",
    "\n",
    "    # Initialize a list to store cosine similarity rows\n",
    "    cosine_similarity_rows = []\n",
    "\n",
    "    # Compute cosine similarity in batches\n",
    "    n_images = embeddings.shape[0]\n",
    "    for i in tqdm(range(0, n_images, batch_size)):\n",
    "        # Get a batch of image embeddings\n",
    "        batch_embeddings = embeddings[i:i+batch_size]\n",
    "        \n",
    "        # Compute cosine similarity for the batch\n",
    "        cosine_similarities = F.cosine_similarity(batch_embeddings.unsqueeze(1), all_concept_embeddings_tensor.unsqueeze(0), dim=2)\n",
    "\n",
    "        # Convert each batch's similarity results into rows\n",
    "        for image_cosine_similarities in cosine_similarities:\n",
    "            cosine_similarity_row = {\n",
    "                concept_value: image_cosine_similarities[i].item()\n",
    "                for i, concept_value in enumerate(all_concept_embeddings.keys())\n",
    "            }\n",
    "            cosine_similarity_rows.append(cosine_similarity_row)\n",
    "        \n",
    "        # Free memory for the next batch\n",
    "        del batch_embeddings, cosine_similarities\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Create a DataFrame where each column corresponds to a concept-value combination\n",
    "    cosine_similarity_df = pd.DataFrame(cosine_similarity_rows)\n",
    "\n",
    "    # Save the cosine similarity results to a CSV file\n",
    "    if only_train:\n",
    "        output_file = f'../Experiments/Cosine_Similarities/{dataset_name}/train_cosine_similarities.csv'\n",
    "    else:\n",
    "        output_file = f'../Experiments/Cosine_Similarities/{dataset_name}/cosine_similarities.csv'\n",
    "    cosine_similarity_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Cosine similarity results for all concepts saved at {output_file} :)\")\n",
    "\n",
    "# Example usage:\n",
    "compute_cosine_sims(dataset_name='CUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59330159-7ec9-4f00-95ac-a32944a4703c",
   "metadata": {},
   "source": [
    "## Plot most activated images for a chosen concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8be546-9cfe-4c93-a276-7b03db9c0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_category(concept_columns):\n",
    "    \"\"\"Helper function to get the user's choice for concept category\"\"\"\n",
    "    print(\"Available concept categories:\")\n",
    "    categories = sorted(set(col.split('::')[0] for col in concept_columns))  # Get unique concept categories\n",
    "    for idx, category in enumerate(categories):\n",
    "        print(f\"{idx + 1}. {category}\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(f\"Enter the number of the concept category you want to choose (1-{len(categories)}): \"))\n",
    "            if 1 <= choice <= len(categories):\n",
    "                return categories[choice - 1], concept_columns\n",
    "            else:\n",
    "                print(\"Invalid choice, please select a number from the list.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "def get_user_concept(category, concept_columns):\n",
    "    \"\"\"Helper function to get the user's choice for specific concept within a category\"\"\"\n",
    "    # Filter concepts based on selected category\n",
    "    concepts = [col for col in concept_columns if col.startswith(category)]\n",
    "    print(f\"\\nAvailable concepts in category '{category}':\")\n",
    "    for idx, concept in enumerate(concepts):\n",
    "        print(f\"{idx + 1}. {concept.split('::')[1]}\")  # Display the specific concept (e.g., 'red', 'cube', etc.)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(f\"Enter the number of the specific concept you want to choose (1-{len(concepts)}): \"))\n",
    "            if 1 <= choice <= len(concepts):\n",
    "                return concepts[choice - 1]\n",
    "            else:\n",
    "                print(\"Invalid choice, please select a number from the list.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "def plot_aligned_images(k=5, dataset_name='CLEVR', only_train=True):\n",
    "    \"\"\" Plot k images most aligned with a user chosen concept from the given dataset \"\"\"\n",
    "    \n",
    "    # Load the cosine similarity results\n",
    "    metadata_df = pd.read_csv(f'Data/{dataset_name}/metadata.csv')\n",
    "    if only_train:\n",
    "        cosine_df = pd.read_csv(f'Experiments/Cosine_Similarities/{dataset_name}/train_cosine_similarities.csv')\n",
    "        metadata_df = metadata_df[metadata_df['split'] == 'train']\n",
    "    else:\n",
    "        cosine_df = pd.read_csv(f'Experiments/Cosine_Similarities/{dataset_name}/cosine_similarities.csv')\n",
    "    \n",
    "    # Get the user's choice of concept category and specific concept\n",
    "    concept_columns = list(cosine_df.columns)\n",
    "    category, concept_columns = get_user_category(concept_columns)  # Get the category first\n",
    "    concept_key = get_user_concept(category, concept_columns)  # Then get the specific concept\n",
    "    \n",
    "    # Sort by cosine similarity and get the top k highest values for the specified concept\n",
    "    top_k_indices = cosine_df.nlargest(k, concept_key).index.tolist()\n",
    "    \n",
    "    # Calculate the number of rows and columns for the plot\n",
    "    n_cols = min(k, 5)  # Up to 5 images per row\n",
    "    n_rows = (k + 4) // 5  # Calculate number of rows needed\n",
    "\n",
    "    # Plot the top k images based on cosine similarity\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten()  # Flatten the axes array to easily index it\n",
    "\n",
    "    plt.suptitle(f\"Top {k} Images with Highest Cosine Similarity to: {concept_key}\", fontsize=16)\n",
    "    for rank, idx in enumerate(top_k_indices):\n",
    "        if rank >= len(axes):  # In case there are fewer images than axes\n",
    "            break\n",
    "        \n",
    "        # Get the image path from metadata\n",
    "        image_filename = metadata_df.iloc[idx].loc['image_path']\n",
    "        img = Image.open(f'Data/{dataset_name}/{image_filename}').convert(\"RGB\")\n",
    "\n",
    "        cos_value = cosine_df.loc[idx, concept_key]\n",
    "        axes[rank].imshow(img)\n",
    "        axes[rank].set_title(f\"Rank {rank+1}\\nCosine = {cos_value:.4f}\")\n",
    "        axes[rank].axis('off')\n",
    "\n",
    "    plt.subplots_adjust(top=0.9, bottom=0.05, hspace=0.3) \n",
    "    plt.tight_layout(pad=0.9, h_pad=0.2)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_aligned_images(dataset_name='CUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c947cb70-515a-4f71-9322-50264c81df6a",
   "metadata": {},
   "source": [
    "## Plot heatmap showing similarities among concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2b221-88bb-41f1-a529-3103f881129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(concept_names, cosine_similarity_matrix, heatmap_title):\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cosine_similarity_matrix, \n",
    "                xticklabels=concept_names, \n",
    "                yticklabels=concept_names, \n",
    "                cmap='coolwarm', \n",
    "                cbar=True, \n",
    "                annot=True, \n",
    "                fmt=\".2f\")\n",
    "\n",
    "    plt.title(heatmap_title)\n",
    "    plt.show()\n",
    "\n",
    "def concept_heatmap(dataset_name='CLEVR', only_train=True):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of cosine similarities between concept embeddings.\n",
    "\n",
    "    \"\"\"\n",
    "    if only_train:\n",
    "        concept_embeddings = torch.load(f'Experiments/Concepts/{dataset_name}/train_concepts.pt')\n",
    "    else:\n",
    "        concept_embeddings = torch.load(f'Experiments/Concepts/{dataset_name}/concepts.pt')\n",
    "    \n",
    "    #sample 10 concepts for visualization purposes\n",
    "    if len(list(concept_embeddings.keys())) > 10:\n",
    "        print(\"Sampling 10 concepts for visualization purposes\")\n",
    "    concept_names = random.sample(list(concept_embeddings.keys()), min(10, len(concept_embeddings)))\n",
    "    concept_names.sort()\n",
    "    \n",
    "    # Get concept names and embeddings\n",
    "    embeddings = torch.stack([concept_embeddings[name] for name in concept_names])\n",
    "    \n",
    "    cosine_similarity_matrix = torch.matmul(embeddings, embeddings.T).cpu().numpy()\n",
    "    create_heatmap(concept_names, cosine_similarity_matrix, 'Cosine Similarity Between Concept Embeddings')\n",
    "\n",
    "\n",
    "def concept_heatmap_groupedby_concept(dataset_name='CLEVR', only_train=True):\n",
    "    if only_train:\n",
    "        concept_embeddings = torch.load(f'Experiments/Concepts/{dataset_name}/train_concepts.pt')\n",
    "    else:\n",
    "        concept_embeddings = torch.load(f'Experiments/Concepts/{dataset_name}/concepts.pt')\n",
    "    \n",
    "    #have user choose concept category\n",
    "    potential_concept_categories = [key for key in concept_embeddings.keys() if key not in ['class', 'image_filename', 'split']]\n",
    "    concept_category = get_user_category(potential_concept_categories)[0]\n",
    "    \n",
    "    #make heatmap just based on those categories\n",
    "    concept_names = [key for key in list(concept_embeddings.keys()) if key.startswith(concept_category)]\n",
    "    embeddings = torch.stack([concept_embeddings[name] for name in concept_names])\n",
    "    \n",
    "    cosine_similarity_matrix = torch.matmul(embeddings, embeddings.T).cpu().numpy()\n",
    "    create_heatmap(concept_names, cosine_similarity_matrix, f'Cosine Similarities Between {concept_category} Concepts')   \n",
    "\n",
    "#concept_heatmap(dataset_name='CLEVR') \n",
    "concept_heatmap_groupedby_concept(dataset_name='CUB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38386047-f357-466d-b76b-f7757db72efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cosine_similarity_histogram(dataset_name='CLEVR', bins=50, images_per_row=3, only_train=True):\n",
    "    \"\"\"\n",
    "    Plots a histogram of cosine similarities for in-sample and out-of-sample concepts.\n",
    "    Displays images in rows of at most `images_per_row` columns.\n",
    "    \"\"\"\n",
    "    # Load files\n",
    "    metadata_df = pd.read_csv(f'Data/{dataset_name}/metadata.csv')\n",
    "    if only_train:\n",
    "        similarity_df = pd.read_csv(f'Experiments/Cosine_Similarities/{dataset_name}/train_cosine_similarities.csv')\n",
    "        metadata_df = metadata_df[metadata_df['split'] == 'train']\n",
    "        metadata_df = metadata_df.reset_index(drop=True)\n",
    "    else:\n",
    "        similarity_df = pd.read_csv(f'Experiments/Cosine_Similarities/{dataset_name}/cosine_similarities.csv')\n",
    "\n",
    "    # Have user choose a concept category\n",
    "    potential_concept_categories = list(similarity_df.columns)\n",
    "    concept_category = get_user_category(potential_concept_categories)[0]\n",
    "    \n",
    "    # Filter concepts that belong to the chosen category\n",
    "    filtered_concepts = [concept_name for concept_name in similarity_df.columns if concept_name.startswith(concept_category)]\n",
    "    \n",
    "    # Determine the number of rows and columns for subplots\n",
    "    num_concepts = len(filtered_concepts)\n",
    "    ncols = images_per_row\n",
    "    nrows = int(np.ceil(num_concepts / ncols))  # Number of rows to fit all plots\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 4, nrows * 4))  # Each image size is 4x4\n",
    "    axes = axes.flatten()  # Flatten to easily index each subplot\n",
    "\n",
    "\n",
    "    for i, concept_name in enumerate(filtered_concepts):\n",
    "        # Extract concept indicator (0/1) and cosine similarity for the concept\n",
    "        concept_labels = metadata_df[concept_name]\n",
    "        concept_similarities = similarity_df[concept_name]\n",
    "\n",
    "        # Separate in-sample and out-of-sample cosine similarities\n",
    "        in_sample_similarities = concept_similarities[concept_labels == 1]\n",
    "        out_of_sample_similarities = concept_similarities[concept_labels == 0]\n",
    "\n",
    "        ax = axes[i]  # Select the current axis to plot on\n",
    "\n",
    "        # Plot the histograms\n",
    "        sns.histplot(in_sample_similarities, bins=bins, color='tab:blue', label='In-Sample', alpha=0.6, kde=False, stat=\"count\", ax=ax)\n",
    "        sns.histplot(out_of_sample_similarities, bins=bins, color='tab:orange', label='Out-of-Sample', alpha=0.6, kde=False, stat=\"count\", ax=ax)\n",
    "\n",
    "        ax.set_xlabel('Cosine Similarity')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(f'{concept_name}', fontsize=10)\n",
    "        ax.legend()\n",
    "\n",
    "    # Remove any empty subplots if there are fewer concepts than grid spaces\n",
    "    for i in range(len(filtered_concepts), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_cosine_similarity_histogram(dataset_name='CLEVR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6befbb3e-74a2-4f06-b5e3-b8f706e4cea9",
   "metadata": {},
   "source": [
    "## Get Info About Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5989ff4a-d51f-4e51-97cc-57de085bf3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'true-false-dataset'\n",
    "metadata = pd.read_csv(f'../Data/{dataset_name}/metadata.csv')\n",
    "print(f\"Number of data samples: {len(metadata)}\")\n",
    "concept_cols = [col for col in metadata.columns if col not in ['image_path', 'class', 'split', 'statement']]                                               \n",
    "print(f\"Number of distinct concepts: {len(concept_cols)}\")\n",
    "concept_categories = set([col.split(\"::\")[0] for col in concept_cols])\n",
    "print(f\"Number of distinct concept categories: {len(concept_categories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3b86ab-b3d6-485b-9fcf-041fd25bf8b1",
   "metadata": {},
   "source": [
    "## Example of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7719ed-0c62-45be-b3b0-ae38d6b4ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"  # Update with your font path if needed\n",
    "font = ImageFont.truetype(font_path, 11)\n",
    "    \n",
    "for _ in range(10):\n",
    "    i = np.random.randint(len(metadata))\n",
    "    print(\"Example:\")\n",
    "    info = metadata.iloc[i]\n",
    "    attributes = [attr for attr in info.index if ((attr not in ['image_path', 'class', 'split']) and (info.loc[attr] == 1))]\n",
    "\n",
    "    image_path = info.loc['image_path']\n",
    "    img = Image.open(f'../Data/{dataset_name}/{image_path}')\n",
    "\n",
    "    # Create a new image with extra space at the bottom to accommodate the text\n",
    "    text_height = 13 # Adjust the height of the text area\n",
    "    new_img = Image.new('RGB', (img.width, img.height + text_height * 15), color=(255, 255, 255))  # Added extra space for multiple lines\n",
    "    new_img.paste(img, (0, 0))\n",
    "\n",
    "    draw = ImageDraw.Draw(new_img)\n",
    "    #font = ImageFont.load_default()\n",
    "\n",
    "    # Prepare the text string with attributes separated by commas\n",
    "    attribute_text = ', '.join(attributes)\n",
    "\n",
    "    # Wrap the text if it's too wide\n",
    "    max_width = img.width   # Keep some padding from the edge\n",
    "    words = attribute_text.split(', ')\n",
    "    lines = []\n",
    "    current_line = \"\"\n",
    "    for word in words:\n",
    "        test_line = f\"{current_line}, {word}\" if current_line else word\n",
    "        test_bbox = draw.textbbox((0, 0), test_line, font=font)\n",
    "        test_width = test_bbox[2] - test_bbox[0]\n",
    "        if test_width <= max_width:\n",
    "            current_line = test_line\n",
    "        else:\n",
    "            lines.append(current_line + \",\")  # Add comma at the end of each line\n",
    "            current_line = word\n",
    "    lines.append(current_line)  # No comma at the end of the last line\n",
    "\n",
    "    # Draw the text on the new image, below the original image\n",
    "    y_offset = img.height + 5  # Start a little below the image\n",
    "    for line in lines:\n",
    "        draw.text((0, y_offset), line, font=font, fill=\"black\")\n",
    "        y_offset += text_height  # Move down for the next line of text\n",
    "\n",
    "    # Show the image with the attributes underneath it\n",
    "    plt.imshow(new_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the new image with attributes underneath\n",
    "    output_image_path = f'../Figs/{dataset_name}/example_{i}.jpg'\n",
    "    new_img.save(output_image_path, dpi=(500, 500))\n",
    "    print(f\"Image with attributes saved to {output_image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4f5d9-a74a-4295-8b05-740be33cf642",
   "metadata": {},
   "source": [
    "## Example of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c401c-8c01-4993-aae0-902d4497e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    i = np.random.randint(len(metadata))\n",
    "    i = 21\n",
    "    info = metadata.iloc[i]\n",
    "    attributes = [attr for attr in info.index if ((attr not in ['class', 'split']) and (info.loc[attr] == 1))]\n",
    "    \n",
    "    if 'true' not in attributes:\n",
    "        attributes.insert(0, 'false')\n",
    "\n",
    "    text = info.loc['statement']\n",
    "\n",
    "    # Set up font size and image properties\n",
    "    font_size = 30  # Larger font size\n",
    "    text_height = font_size + 10  # Space for text and attributes\n",
    "    image_width = 800  # Width of the image\n",
    "    image_height = text_height * (len(attributes) + 1) + 100  # Height for the text and space for attributes\n",
    "    font_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"  # Update with your font path if needed\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    attribute_font = ImageFont.truetype(font_path, 20)\n",
    "\n",
    "    # Create a new image with white background\n",
    "    img = Image.new('RGB', (image_width, image_height), color=(255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Draw the box for the top text\n",
    "    box_margin = 20\n",
    "    box_padding = 5\n",
    "    draw.rectangle([box_margin, box_margin, image_width - box_margin, text_height + box_margin], fill=(230, 230, 230), outline=(0, 0, 0))\n",
    "\n",
    "    # Calculate the width of the text and center it within the box using textbbox\n",
    "    bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    text_width = bbox[2] - bbox[0]\n",
    "    text_x = (image_width - text_width) // 2  # Horizontal center\n",
    "    text_y = box_margin + box_padding  # Vertical position inside the box\n",
    "\n",
    "    # Draw the centered text\n",
    "    draw.text((text_x, text_y), text, font=font, fill=\"black\")\n",
    "\n",
    "    # Prepare the attribute text as a single string and wrap it if necessary\n",
    "    attribute_text = ', '.join(attributes)\n",
    "    max_width = image_width - 2 * box_margin  # Keep padding from the edge\n",
    "\n",
    "    # Wrap the attribute text if it's too wide\n",
    "    words = attribute_text.split(', ')\n",
    "    lines = []\n",
    "    current_line = \"\"\n",
    "    for word in words:\n",
    "        test_line = f\"{current_line}, {word}\" if current_line else word\n",
    "        test_bbox = draw.textbbox((0, 0), test_line, font=font)\n",
    "        test_width = test_bbox[2] - test_bbox[0]\n",
    "        if test_width <= max_width:\n",
    "            current_line = test_line\n",
    "        else:\n",
    "            lines.append(current_line + \",\")  # Add comma at the end of each line\n",
    "            current_line = word\n",
    "    lines.append(current_line)  # No comma at the end of the last line\n",
    "\n",
    "    # Draw the wrapped attribute text below the top text\n",
    "    y_offset = text_height + box_margin + 5  # Position for the first attribute text\n",
    "    for line in lines:\n",
    "        draw.text((box_margin, y_offset), line, font=attribute_font, fill=\"black\")\n",
    "        y_offset += text_height  # Move down for the next line\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the new image with attributes underneath\n",
    "    output_image_path = f'../Figs/{dataset_name}/example_{i}.jpg'\n",
    "    img.save(output_image_path, dpi=(500, 500))\n",
    "    print(f\"Image with attributes saved to {output_image_path}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}