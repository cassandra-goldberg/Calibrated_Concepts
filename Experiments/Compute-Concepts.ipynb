{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f90e159-ca72-4ab1-954b-d6c0bf5b27c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# from collections import defaultdict\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as P\n",
    "# from tqdm import tqdm\n",
    "# import csv\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae252295-5544-4edf-9d18-4b7fe4d503ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CLIP model and processor\n",
    "model_name = \"openai/clip-vit-large-patch14\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6fcab-b01a-4ef8-aa07-f4fa239422b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Embed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42221f99-2305-4095-a5bb-26a1d60772fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_embeddings(images):\n",
    "    inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(pixel_values=inputs['pixel_values']) # Get the class token embedding \n",
    "    return image_features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bce2c00-ce4f-40a3-9a06-404c8a44bf43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:13<00:00, 72.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1001 images.\n",
      "Computing embeddings in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]/opt/conda/envs/rapids/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 11/11 [00:12<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings of shape: torch.Size([1001, 768])\n",
      "Centering embeddings...\n",
      "Embeddings saved to Experiments/Embeddings/CLEVR_embeddings.pt :)\n"
     ]
    }
   ],
   "source": [
    "def compute_image_embeddings(dataset_name = 'CLEVR', batch_size=100):\n",
    "    \"\"\"Computes image embeddings, centers, and normalizes them.\"\"\"\n",
    "    metadata = pd.read_csv(f'Data/{dataset_name}/metadata.csv')\n",
    "    image_paths = metadata['image_path'].tolist()\n",
    "\n",
    "    # Load images\n",
    "    print(\"Loading images...\")\n",
    "    images = []\n",
    "    for image_filename in tqdm(image_paths):\n",
    "        #image_path = os.path.join(image_dir, image_filename)\n",
    "        image = Image.open(image_filename).convert(\"RGB\")\n",
    "        image_tensor = torch.tensor(np.array(image).transpose((2, 0, 1)))  # Convert image to (C, H, W)\n",
    "        images.append(image_tensor)\n",
    "    \n",
    "    print(f\"Loaded {len(images)} images.\")\n",
    "\n",
    "    # Embed images in batches\n",
    "    print(\"Computing embeddings in batches...\")\n",
    "    embeddings = []\n",
    "    n_batches = (len(images) + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        batch_images = images[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_embeddings = get_image_embeddings(batch_images)  # Replace with your actual embedding function\n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    embeddings = torch.cat(embeddings, dim=0).cpu()  # Concatenate all batch embeddings\n",
    "    print(f\"Extracted embeddings of shape: {embeddings.shape}\")\n",
    "\n",
    "    # Center embeddings\n",
    "    print(\"Centering embeddings...\")\n",
    "    mean_embedding = embeddings.mean(dim=0)\n",
    "    centered_embeddings = embeddings - mean_embedding\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    norm_embeddings = centered_embeddings / centered_embeddings.norm(dim=0, keepdim=True)\n",
    "\n",
    "    # Save results\n",
    "    output_file = f'Experiments/Embeddings/{dataset_name}_embeddings.pt'\n",
    "    torch.save(norm_embeddings, output_file)\n",
    "    print(f\"Embeddings saved to {output_file} :)\")\n",
    "\n",
    "compute_image_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c42ec2-03b8-4e20-95b3-b0f64beb9aca",
   "metadata": {},
   "source": [
    "# Compute Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a950a9d7-c48a-4366-8aab-4c50f3210c02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1001, 768])\n"
     ]
    }
   ],
   "source": [
    "def compute_concepts(dataset_name = 'CLEVR'):\n",
    "    # Load the embeddings from the .pt file\n",
    "    embeddings = torch.load(input_embeddings_file)\n",
    "\n",
    "    # Load the color and shape data from the CSV file\n",
    "    metadata_df = pd.read_csv(input_metadata_file)\n",
    "    colors = metadata_df['color'].tolist()\n",
    "    shapes = metadata_df['shape'].tolist()\n",
    "\n",
    "    # Create a dictionary to store the embeddings based on the chosen grouping\n",
    "    concept_embeddings = {}\n",
    "\n",
    "    # Iterate through the data and group embeddings based on the chosen criteria\n",
    "    for idx, embedding in enumerate(embeddings):\n",
    "        color = colors[idx]\n",
    "        shape = shapes[idx]\n",
    "\n",
    "        # Grouping logic\n",
    "        if group_by == 'color':\n",
    "            key = color\n",
    "        elif group_by == 'shape':\n",
    "            key = shape\n",
    "        elif group_by == 'both':\n",
    "            key = f'{color}_{shape}'\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for 'group_by'. Choose from 'color', 'shape', or 'both'.\")\n",
    "\n",
    "        # Add the embedding to the dictionary under the chosen key\n",
    "        if key not in concept_embeddings:\n",
    "            concept_embeddings[key] = []\n",
    "        concept_embeddings[key].append(embedding)\n",
    "\n",
    "    # Calculate the average embedding for each group\n",
    "    avg_concept_embeddings = {}\n",
    "    for key, group_embeddings in concept_embeddings.items():\n",
    "        group_embeddings = torch.stack(group_embeddings)  # Convert to tensor\n",
    "        avg_embedding = group_embeddings.mean(dim=0)  # Calculate the average embedding\n",
    "        avg_embedding = avg_embedding / avg_embedding.norm()  # Normalize the average embedding\n",
    "        avg_concept_embeddings[key] = avg_embedding\n",
    "\n",
    "    # Save the average embeddings to a .pt file\n",
    "    output_file = os.path.join(output_dir, f'concepts_{group_by}.pt')  # Changed to .pt\n",
    "    torch.save(avg_concept_embeddings, output_file)\n",
    "\n",
    "    print(f\"Concept embeddings grouped by {group_by} saved at {output_file} :)\")\n",
    "\n",
    "# Example usage:\n",
    "#compute_concepts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
